{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalization and Discretization\n",
    "\n",
    "Let's use the data we've now parsed and normalize the data, while also coming up with some modular formulas for discretization. First things first, we need to load the dataset into a table and a header:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some useful mysklearn package import statements and reloads\n",
    "import importlib\n",
    "\n",
    "import mysklearn.myutils\n",
    "importlib.reload(mysklearn.myutils)\n",
    "import mysklearn.myutils as myutils\n",
    "\n",
    "import mysklearn.myevaluation\n",
    "importlib.reload(mysklearn.myevaluation)\n",
    "import mysklearn.myevaluation as myevaluation\n",
    "\n",
    "import copy\n",
    "import random\n",
    "\n",
    "header, data = myutils.load_from_file(\"input_data/NCAA_Statistics_Parsed.csv\")\n",
    "data = myutils.convert_to_numeric(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have this, we can move to start doing some funky things with it. I'm going to scale all of these attributes against their domain in the table with the exception of win percentage (the classifier), which will be scaled from 0 to 100 (representing all possible winning %'s. Preliminarily, we will also drop the Team name column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'mysklearn.myutils' has no attribute 'scale_1d'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-6bd25fdef173>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# Now, let's scale our classification.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mmyutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwinp_col\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# Finally, we stitch these back together\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'mysklearn.myutils' has no attribute 'scale_1d'"
     ]
    }
   ],
   "source": [
    "importlib.reload(mysklearn.myutils)\n",
    "import mysklearn.myutils as myutils\n",
    "\n",
    "header, data = myutils.load_from_file(\"input_data/NCAA_Statistics_Parsed.csv\")\n",
    "data = myutils.convert_to_numeric(data)\n",
    "\n",
    "data = myutils.drop_column(data, header, 0)\n",
    "header = header[1:]\n",
    "\n",
    "# Now that we have this, we can scale the data appropriately from 0 to 1 for each attribute. Because we want a\n",
    "# completely winning or losing score to be the min and max, I'm going to grab this column separately.\n",
    "winp_col = myutils.get_column(data, header, \"Win Percentage\")\n",
    "data = myutils.drop_column(data, header, \"Win Percentage\")\n",
    "cut_header = header[:-1]\n",
    "\n",
    "# Okay, now we can move to scaling. Let's start with the X attributes, which will be dead simple:\n",
    "X_mins, X_maxs = myutils.scale(data)\n",
    "\n",
    "# Now, let's scale our classification.\n",
    "myutils.scale_1d(winp_col, 0, 100)\n",
    "\n",
    "# Finally, we stitch these back together\n",
    "for i in range(len(data)):\n",
    "    data[i].append(winp_col[i])\n",
    "\n",
    "# Now let's save it to a new csv file to confirm our work's efficacy\n",
    "myutils.save_to_file(header, data, \"input_data/NCAA_Statistics_Normalized.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And with that, we've completed the normalization process. Now let's define some modular splitting functions in myutils to discretize the data into useful buckets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before moving forward, I want to mention my strategy here. I've made a function that modularizes the number of bins; Nothing special, for certain, but it does allow us to operate with multiple files here. I'm going to use the num_bins variable to, in part, name our output files, so we can operate well here. Of course this is unnecessary in our classification method, as we can simply call the function, but for testing and display purposes, that's the plan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(mysklearn.myutils)\n",
    "import mysklearn.myutils as myutils\n",
    "\n",
    "header, data = myutils.load_from_file(\"input_data/NCAA_Statistics_Normalized.csv\")\n",
    "data = myutils.convert_to_numeric(data)\n",
    "\n",
    "# 1) Using the \"22222\" method. Generally too unspecific to help.\n",
    "num_bins = [2, 2, 2, 2, 2]\n",
    "data = myutils.discretize(data, num_bins)\n",
    "num_bins = myutils.convert_to_lexical(num_bins, table_dimensions=1)\n",
    "myutils.save_to_file(header, data, \"input_data/NCAA_Statistics_%s.csv\" % \"\".join(num_bins))\n",
    "\n",
    "header, data = myutils.load_from_file(\"input_data/NCAA_Statistics_Normalized.csv\")\n",
    "data = myutils.convert_to_numeric(data)\n",
    "\n",
    "# 2) Using the same, but overriding the winning percentage cutoffs.\n",
    "num_bins = [2, 2, 2, 2, 2]\n",
    "data = myutils.discretize(data, num_bins, cutoffs=[None, None, None, None, [0, 0.5, 1.0]])\n",
    "num_bins = myutils.convert_to_lexical(num_bins, table_dimensions=1)\n",
    "myutils.save_to_file(header, data, \"input_data/NCAA_Statistics_%s_alt.csv\" % \"\".join(num_bins))\n",
    "\n",
    "header, data = myutils.load_from_file(\"input_data/NCAA_Statistics_Normalized.csv\")\n",
    "data = myutils.convert_to_numeric(data)\n",
    "\n",
    "# 3) Using more discretization labels, with 4 classifications.\n",
    "# NOTE: I don't want to rely on Scoring Margin too much here; I think it's an easy-win button here.\n",
    "num_bins = [2, 4, 4, 4, 4]\n",
    "data = myutils.discretize(data,num_bins)\n",
    "num_bins = myutils.convert_to_lexical(num_bins, table_dimensions=1)\n",
    "myutils.save_to_file(header, data, \"input_data/NCAA_Statistics_%s.csv\" % \"\".join(num_bins))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By this point, we have an awesome discretization function. Let's make it even more modular so we can generate a whole ton of them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(mysklearn.myutils)\n",
    "import mysklearn.myutils as myutils\n",
    "\n",
    "def print_csv(num_bins):\n",
    "    header, data = myutils.load_from_file(\"input_data/NCAA_Statistics_Normalized.csv\")\n",
    "    data = myutils.convert_to_numeric(data)\n",
    "\n",
    "    data = myutils.discretize(data, num_bins, cutoffs=[None, None, None, None, [0.0, 0.35, 0.50, 0.65, 1.0]])\n",
    "    num_bins = myutils.convert_to_lexical(num_bins, table_dimensions=1)\n",
    "    myutils.save_to_file(header, data, \"input_data/NCAA_Statistics_%s.csv\" % \"\".join(num_bins))\n",
    "    \n",
    "# Now for a big boy nested loop...\n",
    "for a in range(2, 6):\n",
    "    for b in range(2, 6):\n",
    "        for c in range(2, 6):\n",
    "            for d in range(2, 6):\n",
    "                print_csv([a, b, c, d, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
